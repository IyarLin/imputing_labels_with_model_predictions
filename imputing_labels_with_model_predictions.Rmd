---
title: "Does imputing model labels using the model predictions can improve it's performance?"
author: "Iyar Lin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  github_document:
    toc: true
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = F, message = F, cache = F)
set.seed(1)
options(scipen = 999)

packages <- c(
  "tidyverse", # best thing that happend to me
  "pander"  # table rendering
)

sapply(
  packages,
  function(x) if (!require(x, character.only = TRUE, quietly = T)) {
      install.packages(x, quiet = T, verbose = F)
      library(x, character.only = T, quietly = T, verbose = F)
    }
)
```

# Motivation

In some scenarios a data scientist may want to train a model for which there exists an abundance of observations, of which only a small fraction is labeled, making the sample size available to train the model rather small. Although there's plenty of literature on the subject (e.g. "Active learning", "Semi-supervised learning" etc) one may be tempted (maybe due to fast approaching deadlines) to impute the unlabeled data with the values predicted by a model trained on the labeled data. 

While for some the above suggestion might seem simply incorrect, I have encountered such suggestions on several occasions and had a hard time refuting them. To make sure it wasn't just the type of places I work at I went and asked around in 2 Israeli (sorry non Hebrew readers) machine learning oriented Facebook groups about their opinion: [Machine & Deep learning Israel](https://www.facebook.com/groups/543283492502370/permalink/1158551544308892/) and [Statistics and probability group](https://www.facebook.com/groups/statprob/permalink/767687730239611/). While many were referring me to methods discussed in the literature, almost no one indicated the proposed method was utterly wrong. 

This post is intended to prove to myself and anyone else interested that imputing labels with your model predictions can't help and in some situations may even hurt your model performance.

# The proposed procedure

Formally speaking, given feature matrix $X$, labels $y$ and a set of indices $I$ for which the labels are missing the procedure discussed is:

1. Train "small" model $\hat{f_S}(x)$ based on the observations $\{X_i,y_i\}, \, i \notin I$
1. Predict the labels $\hat{y}_i=\hat{f_S}(x_i), \, i \in I$
1. Train "large" model $\hat{f_L}(x)$ on the full sample $\{X, y\}$

# Simulation study  
## The benign case  

First, simulate our full feature matrix $X$:

```{r simulate data}
# I'm going to use a correlation matrix from which I'll simulate the feature matrix X. 
# I'll use the mtcars dataset to get some "real world" parameter values. 
# In the next few lines of code I closely follow this great post on r-bloggers: https://www.r-bloggers.com/simulating-data-following-a-given-covariance-structure/

correlation_matrix <- cor(mtcars %>% select(mpg, disp, hp, wt))

# Cholesky decomposition
L <- chol(correlation_matrix)
nvars <- dim(L)[1]
nobs <- 1000
# Random variables that follow an M correlation matrix:
sim_data = setNames(object = as.data.frame(t(t(L) %*% matrix(rnorm(nvars*nobs), nrow=nvars, ncol=nobs))), 
                    nm = c("pred1", "pred2", "pred3", "pred4"))
```

Next we'll assume a linear regression model and simulate the labels $y$ for the full dataset, then split it to train and test sets:

```{r simulate labels}
beta <- setNames(object = c(0.3, -0.2, 0.5, 0.3, -0.6), nm = c("beta0", "beta1", "beta2", "beta3", "beta4"))
sim_data$y <- as.numeric(cbind(1, as.matrix(sim_data)) %*% beta) + rnorm(nobs)

train_data <- sim_data[1:800, ]; test_data <- sim_data[801:1000, ]
```

Now we'll start implementing the above procedure, assuming the data scientist has labels for the first 20 observations.

```{r perform procedure on first 20 points}
available_train_data <- train_data; available_train_data$y[21:800] <- NA
small_model <- lm(y ~ ., data = available_train_data)
imputed_labels <- predict(small_model, available_train_data[21:800, ])
available_train_data$y[21:800] <- imputed_labels
large_model <- lm(y ~ ., data = available_train_data)
```

Next we'll measure the performance of the small and large models on the test set:

```{r measure perforamce on test set, results = "asis"}
RMSE <- function(y, y_hat) sqrt(mean((y - y_hat)^2))
small_model_performance <- RMSE(y = test_data$y, y_hat = predict(small_model, test_data))
large_model_performance <- RMSE(y = test_data$y, y_hat = predict(large_model, test_data))

pandoc.table(data.frame(model = c("small model", "large model"), RMSE = c(small_model_performance, large_model_performance)))
```

We can see that imputing the data didn't improve neither did it hurt the model performance.

## The ugly case

There are cases however where the above procedure can be harmful. In some organizations one has access to man power that can manually tag observations. Since such tagging process is costly in time and money, one may try to "enhance" it with additional tags from the small model using the above procedure. In the simulation below I demonstrate this has the effect of "diluting" the information gained from the manual tagging process thus reducing its effectiveness.

```{r new lage model performance, results = "asis"}
available_train_data <- train_data; available_train_data$y[21:800] <- NA
available_train_data$y[21:200] <- train_data$y[21:200] # this represents manual tagging of additional 180 observations
new_small_model <- lm(y ~ ., data = available_train_data)
imputed_labels <- predict(small_model, available_train_data[101:800, ])
available_train_data$y[101:800] <- imputed_labels
new_large_model <- lm(y ~ ., data = available_train_data)

new_small_model_performance <- RMSE(y = test_data$y, y_hat = predict(new_small_model, test_data))
new_large_model_performance <- RMSE(y = test_data$y, y_hat = predict(new_large_model, test_data))

pandoc.table(data.frame(model = c("new small model", "new large model"), RMSE = c(new_small_model_performance, new_large_model_performance)))
```

We can see that while both models improved, the model with the imputed labels improved less.

# Conclusion

If you face a problem of sparse labels and want to improve your model, try some of the methodologies developed to deal with that situation. Don't impute the labels using the model you're trying to improve.